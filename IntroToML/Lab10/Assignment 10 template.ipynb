{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "For this assignment you need to:\n",
    "    Prepare the data - stopwords, stemming\n",
    "    Apply both: bag of words and tfIdf\n",
    "    Apply Logistic Regression, SVM, and NaiveBayes for the ready dataset\n",
    "    Compare accuracies for bag of words and tfidf with all the algorithms above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The sentiment labels are:\n",
    "\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.tsv\", delimiter='\t', header=0)\n",
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.tsv\", delimiter='\t', header=0)\n",
    "data = data[0:4990]\n",
    "X = data[['PhraseId', 'SentenceId', 'Phrase']]\n",
    "y = data['Sentiment']\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "data['Phrase'] = data['Phrase'].apply(lambda x: \" \".join([item for item in (x.lower()).split(\" \") if item not in stopWords]))\n",
    "data['Phrase'] = data['Phrase'].apply(lambda x: ps.stem(x))\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words: (4990, 2071)\n",
      "Words Vectorizer: (4990, 2071)\n"
     ]
    }
   ],
   "source": [
    "wordsbag = CountVectorizer()\n",
    "X_wb = wordsbag.fit_transform(data['Phrase'])\n",
    "print(\"Bag of Words:\", X_wb.shape)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfIdf = vectorizer.fit_transform(data['Phrase'])\n",
    "print(\"Words Vectorizer:\", X_tfIdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrainLabels, yTestLabels = train_test_split(X_wb, y,test_size=0.2, random_state=123)\n",
    "trainBW, testBW, trainLabelsBW, testLabelsBW = train_test_split(xTest, yTestLabels, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrainLabels, yTestLabels = train_test_split(X_tfIdf, y,test_size=0.2, random_state=123)\n",
    "trainTFIDF, testTFIDF, trainLabelsTFIDF, testLabelsTFIDF = train_test_split(xTest, yTestLabels, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for wordsbag with Logistic Regression:\n",
      "0.6766666666666666\n",
      "Accuracy for wordsbag with SVC\n",
      "0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(trainBW, trainLabelsBW)\n",
    "predict = clf.predict(testBW)\n",
    "print(\"Accuracy for wordsbag with Logistic Regression:\")\n",
    "print(accuracy_score(testLabelsBW, predict))\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(trainBW, trainLabelsBW)\n",
    "predict = svc.predict(testBW)\n",
    "print(\"Accuracy for wordsbag with SVC\")\n",
    "print(accuracy_score(testLabelsBW, predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for vectorizer with Logistic Regression:\n",
      "0.64\n",
      "Accuracy for vectorizer with SVC\n",
      "0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "lg.fit(trainTFIDF, trainLabelsTFIDF)\n",
    "predict = lg.predict(testTFIDF)\n",
    "print(\"Accuracy for vectorizer with Logistic Regression:\")\n",
    "print(accuracy_score(testLabelsTFIDF, predict))\n",
    "\n",
    "\n",
    "svcc = SVC()\n",
    "svcc.fit(trainTFIDF, trainLabelsTFIDF)\n",
    "predict = svcc.predict(testTFIDF)\n",
    "print(\"Accuracy for vectorizer with SVC\")\n",
    "print(accuracy_score(testLabelsTFIDF, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "work\n",
      "and\n",
      "no\n",
      "play\n",
      "make\n",
      "jack\n",
      "dull\n",
      "boy\n",
      ".\n",
      "all\n",
      "work\n",
      "and\n",
      "no\n",
      "play\n",
      "make\n",
      "jack\n",
      "a\n",
      "dull\n",
      "boy\n",
      ".\n",
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download()\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    " \n",
    "for w in words:\n",
    "    print(ps.stem(w)) #stemming\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w) #stopwords\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
